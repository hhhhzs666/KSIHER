# KSIHER
Code and data for the paper "A General Coarse-to-fine Approach for Knowledge Selection based on Iterative Hybrid Encoding and Re-ranking"

## Abstract

Knowledge selection is a challenging task which often deals with semantic drift issues when knowledge is retrieved based on semantic similarity between a fact and a question. This paper presents a coarse-to-fine framework for alleviating these issues. Firstly, a sparse encoder and a dense encoder are coupled iteratively to retrieve fact candidates from a large-scale knowledge base. Secondly, a pre-trained language model with two rounds of fine-tuning using results of the sparse and dense encoders is used to re-rank fact candidates. Finally, top-k facts are selected by a specific re-ranker. An extensive evaluation is performed on two textual inference datasets and one knowledge-grounded question answering dataset. Experimental results show that: (1) the coarse-to-fine framework can improve the performance of knowledge selection by reducing the semantic drift; (2) the proposed approach is scalable and produces outstanding results on the benchmark datasets.

## Setup:

Install all packages:

`pip install -r requirements.txt`

## Hybrid-Encoder:

To reproduce our experiments, train the model and store it in `./models`, build training data stored in `./entailmentbank/train`, outputs stored in `./entailmentbank/outputs`

First you can regenerate the training-set using the `build_hybrid-encoder_train_data.py` script and output the inference chains stored in `./entailmentbank/train/triplets_data.csv`.

**Training:**

If you want to train the dense encoder from scratch, you can use the released `train_hybrid-encoder.py` script. This will create a new [Sentence-BERT](https://www.sbert.net/) model (`bert-base-uncased`) and fine-tune it on the inference chains stored in `./entailmentbank/train/triplets_data.csv` via MultipleNegativesRankingLoss.

## Rerank-Encoder

The pre-trained TANDA model used in our experiments can be downloaded [here!](https://d3t7erp6ge410c.cloudfront.net/tanda-aaai-2020/models/tanda_roberta_base_asnq.tar)

To reproduce our experiments, download the model and store it in `./models`.

Then you can regenerate the training-set using the `build_1th_rerank-encoder_train_data.py` script and output the triplets stored in `./entailmentbank/train/rerank_train_1th.csv`.

**The first round of training:**

You can use the released `train_rerank-encoder.py` script. This will use a new TANDA model and fine-tune it on the triplets stored in `./entailmentbank/train/rerank_train_1th.csv` .

**The second round of training:**

First use the released dataset `./entailmentbank/data/hypotheses_train.json` to obtain the top-N candidate facts generated by the Knowledge Selection based on Iterative Hybrid Encoding `./explanation_regeneration_experiment.py`. And output the top-N candidate facts stored in `entailmentbank/outputs/pre_train_top50.json` .

Then you can use the released `build_2th_rerank-encoder_train_data.py` script on the top-N candidate facts stored in `entailmentbank/outputs/pre_train_top50.json` and output the second round of fine tuning data `./entailmentbank/train/rerank_train_2th.csv`.

And then you can continue use the released `train_rerank-encoder.py` script. This will use the first round of fine-tuned model and continue fine-tune it on the second round of tuning data stored in `./entailmentbank/train/rerank_train_2th.csv` .

##  Knowledge Selection based on Iterative Hybrid Encoding Experiment:

Put the trained models into the `./models` folder, run the following command to start the experiment:

`python ./explanation_regeneration_experiment.py`

This will create the [FAISS](https://faiss.ai/) index and perform multi-hop inference using Iterative Hybrid Encoding

##  Knowledge Selection based on Re-ranking Experiment:

Put the trained models into the `./models` folder, run the following command to start the experiment:

`python ./rerank.py`

##  Compute the F1 score:

Once the experiment is completed, you can compute the F1 score using the following command:

`python ./F1_score.py`

##  Acknowledgement

Thanks to [Valentino et al](https://github.com/ai-systems/hybrid_autoregressive_inference) for thrie research. Our proposed approach is built upon the work
of [Hybrid Autoregressive Inference for Scalable Multi-Hop Explanation Regeneration](https://arxiv.org/abs/2107.11879). 
