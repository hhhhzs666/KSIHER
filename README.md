# KSIHER
Code and data for the paper "A General Coarse-to-fine Approach for Knowledge Selection based on Iterative Hybrid Encoding and Re-ranking"

## Setup:

Install the [sentence-transformers](https://www.sbert.net/) package:

`pip install -U sentence-transformers`

Install the [faiss-gpu](https://pypi.org/project/faiss-gpu/) package:

`pip install faiss-gpu`

## Bi-Encoder:

To reproduce our experiments, download the model and store it in `./models`.

**Training:**

If you want to train the dense encoder from scratch, you can use the released `train_bi-encoder.py` script. This will create a new [Sentence-BERT](https://www.sbert.net/) model (`bert-base-uncased`) and fine-tune it on the inference chains stored in `./entailmentbank/train/triplets_data.csv` via MultipleNegativesRankingLoss.

If needed, you can regenerate the training-set using the `build_bi-encoder_train_data.py` script.

## Cross-Encorder

The pre-trained TANDA cross-encoder used in our experiments can be downloaded [here!](https://d3t7erp6ge410c.cloudfront.net/tanda-aaai-2020/models/tanda_roberta_base_asnq.tar)

To reproduce our experiments, download the model and store it in `./models`.

**Training:**

*1th*

you can use the released `train_cross-encoder.py` script. This will use a new TANDA model and fine-tune it on the triplets stored in `./entailmentbank/train/cross_train_1th.csv` .

If needed, you can regenerate the training-set using the `build_1th_cross-encoder_train_data.py` script.

*2th*

Use the train-set `./entailmentbank/data/hypotheses_train.json` obtain the top-N candidate facts generated by the Knowledge Selection based on Iterative Hybrid Encoding . Then you can use the released `build_2th_cross-encoder_train_data.py` script and obtain the second round of fine tuning data.

And then you can continue use the released `train_cross-encoder.py` script. This will use a fine-tuned 1th model and fine-tune it on the triplets stored in `./entailmentbank/train/cross_train_2th.csv` .

##  Knowledge Selection based on Iterative Hybrid Encoding Experiment:

Put the trained models into the `./models` folder, run the following command to start the experiment:

`python ./explanation_regeneration_experiment.py`

This will create the [FAISS](https://faiss.ai/) index and perform multi-hop inference using Iterative Hybrid Encoding

##  Knowledge Selection based on Re-ranking Experiment:

Put the trained models into the `./models` folder, run the following command to start the experiment:

`python ./rerank.py`

##  Compute the F1 score:

Once the experiment is completed, you can compute the F1 score using the following command:

`python ./F1_score.py`
