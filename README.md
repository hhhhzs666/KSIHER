# KSIHER
Code and data for the paper "A General Coarse-to-fine Approach for Knowledge Selection based on Iterative Hybrid Encoding and Re-ranking"

## Abstract

Knowledge selection is a challenging task which often deals with semantic drift issues when knowledge is retrieved based on semantic similarity between a fact and a question. This paper presents a coarse-to-fine framework for alleviating these issues. Firstly, a sparse encoder and a dense encoder are coupled iteratively to retrieve fact candidates from a large-scale knowledge base. Secondly, a pre-trained language model with two rounds of fine-tuning using results of the sparse and dense encoders is used to re-rank fact candidates. Finally, top-k facts are selected by a specific re-ranker. An extensive evaluation is performed on two textual inference datasets and one knowledge-grounded question answering dataset. Experimental results show that: (1) the coarse-to-fine framework can improve the performance of knowledge selection by reducing the semantic drift; (2) the proposed approach is scalable and produces outstanding results on the benchmark datasets.

## Setup:

Install the [sentence-transformers](https://www.sbert.net/) package:

`pip install -U sentence-transformers`

Install the [faiss-gpu](https://pypi.org/project/faiss-gpu/) package:

`pip install faiss-gpu`

## Hybrid-Encoder:

To reproduce our experiments, download the model and store it in `./models`.

**Training:**

If you want to train the dense encoder from scratch, you can use the released `train_bi-encoder.py` script. This will create a new [Sentence-BERT](https://www.sbert.net/) model (`bert-base-uncased`) and fine-tune it on the inference chains stored in `./entailmentbank/train/triplets_data.csv` via MultipleNegativesRankingLoss.

If needed, you can regenerate the training-set using the `build_bi-encoder_train_data.py` script.

## Rerank-Encoder

The pre-trained TANDA cross-encoder used in our experiments can be downloaded [here!](https://d3t7erp6ge410c.cloudfront.net/tanda-aaai-2020/models/tanda_roberta_base_asnq.tar)

To reproduce our experiments, download the model and store it in `./models`.

**1th-Training:**

you can use the released `train_cross-encoder.py` script. This will use a new TANDA model and fine-tune it on the triplets stored in `./entailmentbank/train/cross_train_1th.csv` .

If needed, you can regenerate the training-set using the `build_1th_cross-encoder_train_data.py` script.

**2th-Training:**

Use the train-set `./entailmentbank/data/hypotheses_train.json` obtain the top-N candidate facts generated by the Knowledge Selection based on Iterative Hybrid Encoding . Then you can use the released `build_2th_cross-encoder_train_data.py` script and obtain the second round of fine tuning data.

And then you can continue use the released `train_cross-encoder.py` script. This will use a fine-tuned 1th model and fine-tune it on the triplets stored in `./entailmentbank/train/cross_train_2th.csv` .

##  Knowledge Selection based on Iterative Hybrid Encoding Experiment:

Put the trained models into the `./models` folder, run the following command to start the experiment:

`python ./explanation_regeneration_experiment.py`

This will create the [FAISS](https://faiss.ai/) index and perform multi-hop inference using Iterative Hybrid Encoding

##  Knowledge Selection based on Re-ranking Experiment:

Put the trained models into the `./models` folder, run the following command to start the experiment:

`python ./rerank.py`

##  Compute the F1 score:

Once the experiment is completed, you can compute the F1 score using the following command:

`python ./F1_score.py`
